{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "Data Science Workflow:\n",
    "1. Start with a question \n",
    "2. Get & Clean the data   \n",
    "3. Perform EDA\n",
    "4. Apply Techniques\n",
    "    - **Input:** clean data, plus we have verified that the data makes sense\n",
    "    - **NLP Techniques:** advanced analysis techniques, in this case, they are specifically designed for text data\n",
    "    - **Output:** additional insights about our data to help us answer our question, \"how is Ali Wong different?\"\n",
    "5. Share Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "- **Input:** A document-term matrix. Each topic will consist of a set of wprds where order does not matter, so we are going to start with the bag of words format.\n",
    "- **genism:** genism is a Python toolkit built by Radim Rehurek specifically for topic modeling. We are going to a popular topic modeling technique called Latent Dirichlet Allocation (LDA). We are also going to use **nltk** for some parts-of-speech tagging.\n",
    "- **Output:** Our goal is to find themes across various comedy routines, and see which comedians tend to talk about which themes.\n",
    "\n",
    "### Latent Dirichlet Allocation\n",
    "Latent -- > hidden <br>\n",
    "Dirichlet --> types of probability distribution\n",
    "\n",
    "### How LDA works?\n",
    "- **Goal:** - You want LDA to learn the topic mix in each document, and the word mix in each topic\n",
    "- **Input:** - Document-Term Matrix, number of topics, number of iterations\n",
    "- genism will go through the process of finding the best word idstribution for each topic and best topic distribution for each document.\n",
    "- **Output:** *The top words in each topic*. It is your job as a human to interpret this and see if the results makes sense. If not, try altering the parameters - terms in the document-term matrix, number of topics, number of iterations, etc, *Stop when the topics make sense*.\n",
    "- This is a probabilisatic approach to topic modelling. There are also matriz factorization techniques for topic modeling such as **Latent Semantic Indexing (LSI)** and **Non-Negative Matrix Factorization (NMF)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Topic Modelling -  Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>...</th>\n",
       "      <th>zee</th>\n",
       "      <th>zen</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 7465 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aaah  aah  abc  \\\n",
       "ali           0             0                 0          0     0    0    1   \n",
       "anthony       0             0                 0          0     0    0    0   \n",
       "bill          1             0                 0          0     0    0    0   \n",
       "bo            0             1                 1          1     0    0    0   \n",
       "dave          0             0                 0          0     1    0    0   \n",
       "hasan         0             0                 0          0     0    0    0   \n",
       "jim           0             0                 0          0     0    0    0   \n",
       "joe           0             0                 0          0     0    0    0   \n",
       "john          0             0                 0          0     0    0    0   \n",
       "louis         0             0                 0          0     0    3    0   \n",
       "mike          0             0                 0          0     0    0    0   \n",
       "ricky         0             0                 0          0     0    0    0   \n",
       "\n",
       "         abcs  ability  abject  ...  zee  zen  zeppelin  zero  zillion  \\\n",
       "ali         0        0       0  ...    0    0         0     0        0   \n",
       "anthony     0        0       0  ...    0    0         0     0        0   \n",
       "bill        1        0       0  ...    0    0         0     1        1   \n",
       "bo          0        1       0  ...    0    0         0     1        0   \n",
       "dave        0        0       0  ...    0    0         0     0        0   \n",
       "hasan       0        0       0  ...    2    1         0     1        0   \n",
       "jim         0        0       0  ...    0    0         0     0        0   \n",
       "joe         0        0       0  ...    0    0         0     0        0   \n",
       "john        0        0       0  ...    0    0         0     0        0   \n",
       "louis       0        0       0  ...    0    0         0     2        0   \n",
       "mike        0        0       0  ...    0    0         2     1        0   \n",
       "ricky       0        1       1  ...    0    0         0     0        0   \n",
       "\n",
       "         zombie  zombies  zoning  zoo  éclair  \n",
       "ali           1        0       0    0       0  \n",
       "anthony       0        0       0    0       0  \n",
       "bill          1        1       1    0       0  \n",
       "bo            0        0       0    0       0  \n",
       "dave          0        0       0    0       0  \n",
       "hasan         0        0       0    0       0  \n",
       "jim           0        0       0    0       0  \n",
       "joe           0        0       0    0       0  \n",
       "john          0        0       0    0       1  \n",
       "louis         0        0       0    0       0  \n",
       "mike          0        0       0    0       0  \n",
       "ricky         0        0       0    1       0  \n",
       "\n",
       "[12 rows x 7465 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ali</th>\n",
       "      <th>anthony</th>\n",
       "      <th>bill</th>\n",
       "      <th>bo</th>\n",
       "      <th>dave</th>\n",
       "      <th>hasan</th>\n",
       "      <th>jim</th>\n",
       "      <th>joe</th>\n",
       "      <th>john</th>\n",
       "      <th>louis</th>\n",
       "      <th>mike</th>\n",
       "      <th>ricky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ali  anthony  bill  bo  dave  hasan  jim  joe  john  louis  \\\n",
       "aaaaah              0        0     1   0     0      0    0    0     0      0   \n",
       "aaaaahhhhhhh        0        0     0   1     0      0    0    0     0      0   \n",
       "aaaaauuugghhhhhh    0        0     0   1     0      0    0    0     0      0   \n",
       "aaaahhhhh           0        0     0   1     0      0    0    0     0      0   \n",
       "aaah                0        0     0   0     1      0    0    0     0      0   \n",
       "\n",
       "                  mike  ricky  \n",
       "aaaaah               0      0  \n",
       "aaaaahhhhhhh         0      0  \n",
       "aaaaauuugghhhhhh     0      0  \n",
       "aaaahhhhh            0      0  \n",
       "aaah                 0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, \n",
    "# from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3680: 'ladies',\n",
       " 2750: 'gentlemen',\n",
       " 7246: 'welcome',\n",
       " 6269: 'stage',\n",
       " 156: 'ali',\n",
       " 7353: 'wong',\n",
       " 3095: 'hi',\n",
       " 3074: 'hello',\n",
       " 6658: 'thank',\n",
       " 1354: 'coming',\n",
       " 5907: 'shit',\n",
       " 1041: 'cause',\n",
       " 4795: 'pee',\n",
       " 4200: 'minutes',\n",
       " 2270: 'everybody',\n",
       " 6971: 'um',\n",
       " 2294: 'exciting',\n",
       " 1706: 'day',\n",
       " 7419: 'year',\n",
       " 6921: 'turned',\n",
       " 7428: 'yes',\n",
       " 282: 'appreciate',\n",
       " 6967: 'uh',\n",
       " 6614: 'tell',\n",
       " 2764: 'getting',\n",
       " 4564: 'older',\n",
       " 2780: 'girl',\n",
       " 411: 'automatic',\n",
       " 6692: 'thought',\n",
       " 2664: 'fuck',\n",
       " 6372: 'straight',\n",
       " 3491: 'jealous',\n",
       " 2596: 'foremost',\n",
       " 4145: 'metabolism',\n",
       " 2783: 'girls',\n",
       " 2101: 'eat',\n",
       " 6018: 'sixpack',\n",
       " 6662: 'thatthat',\n",
       " 550: 'beautiful',\n",
       " 3364: 'inner',\n",
       " 6680: 'thigh',\n",
       " 1244: 'clearance',\n",
       " 2428: 'feet',\n",
       " 6674: 'theres',\n",
       " 3227: 'huge',\n",
       " 2707: 'gap',\n",
       " 3814: 'light',\n",
       " 5031: 'potential',\n",
       " 5259: 'radiating',\n",
       " 6709: 'throughand',\n",
       " 6052: 'sleep',\n",
       " 3377: 'insomnia',\n",
       " 194: 'ambien',\n",
       " 1991: 'download',\n",
       " 4101: 'meditation',\n",
       " 4518: 'oasis',\n",
       " 4958: 'podcast',\n",
       " 948: 'calm',\n",
       " 1119: 'chatter',\n",
       " 5392: 'regret',\n",
       " 5459: 'resentment',\n",
       " 2379: 'family',\n",
       " 1294: 'cluttering',\n",
       " 4182: 'mind',\n",
       " 3847: 'lives',\n",
       " 122: 'ahead',\n",
       " 3222: 'hpv',\n",
       " 4783: 'peace',\n",
       " 4452: 'night',\n",
       " 4561: 'ok',\n",
       " 1338: 'come',\n",
       " 2670: 'fucking',\n",
       " 3893: 'loser',\n",
       " 5703: 'says',\n",
       " 3898: 'lot',\n",
       " 4125: 'men',\n",
       " 6995: 'undetectable',\n",
       " 5327: 'really',\n",
       " 2665: 'fucked',\n",
       " 2767: 'ghost',\n",
       " 3372: 'inside',\n",
       " 4127: 'mens',\n",
       " 721: 'bodies',\n",
       " 734: 'boo',\n",
       " 7347: 'womens',\n",
       " 1944: 'doctor',\n",
       " 6765: 'told',\n",
       " 6374: 'strains',\n",
       " 3631: 'kind',\n",
       " 6920: 'turn',\n",
       " 1070: 'cervical',\n",
       " 965: 'cancer',\n",
       " 722: 'body',\n",
       " 3043: 'heal',\n",
       " 3080: 'helpful',\n",
       " 517: 'basically',\n",
       " 1851: 'die',\n",
       " 5062: 'presence',\n",
       " 7342: 'wolverine',\n",
       " 654: 'bitches',\n",
       " 3634: 'kindle',\n",
       " 6922: 'turning',\n",
       " 5808: 'selfhelp',\n",
       " 3800: 'library',\n",
       " 3402: 'interested',\n",
       " 739: 'books',\n",
       " 5864: 'shades',\n",
       " 2886: 'grey',\n",
       " 3808: 'lifechanging',\n",
       " 3958: 'magic',\n",
       " 6724: 'tidying',\n",
       " 1741: 'declutter',\n",
       " 3152: 'home',\n",
       " 42: 'achieve',\n",
       " 4613: 'optimum',\n",
       " 3784: 'level',\n",
       " 6446: 'success',\n",
       " 3192: 'horrible',\n",
       " 4833: 'person',\n",
       " 2997: 'happy',\n",
       " 3078: 'help',\n",
       " 6779: 'tony',\n",
       " 5551: 'robbins',\n",
       " 4108: 'mei',\n",
       " 3136: 'hoarding',\n",
       " 5103: 'problem',\n",
       " 3183: 'hoping',\n",
       " 1058: 'center',\n",
       " 5104: 'problems',\n",
       " 2813: 'goes',\n",
       " 428: 'away',\n",
       " 1887: 'disappear',\n",
       " 4250: 'mom',\n",
       " 7375: 'world',\n",
       " 1523: 'country',\n",
       " 6586: 'taught',\n",
       " 6710: 'throw',\n",
       " 1846: 'dictators',\n",
       " 4663: 'overtake',\n",
       " 6112: 'snatch',\n",
       " 7217: 'wealth',\n",
       " 601: 'better',\n",
       " 3139: 'hold',\n",
       " 5482: 'retainer',\n",
       " 2845: 'grade',\n",
       " 2979: 'handy',\n",
       " 5943: 'shovel',\n",
       " 912: 'busy',\n",
       " 6429: 'stuffing',\n",
       " 2815: 'gold',\n",
       " 914: 'butt',\n",
       " 5624: 'running',\n",
       " 1369: 'communiststhe',\n",
       " 5660: 'san',\n",
       " 2625: 'francisco',\n",
       " 6903: 'trying',\n",
       " 5517: 'rid',\n",
       " 7383: 'worst',\n",
       " 2323: 'experience',\n",
       " 3807: 'life',\n",
       " 2182: 'emotional',\n",
       " 5746: 'screaming',\n",
       " 2464: 'fighting',\n",
       " 7424: 'yelling',\n",
       " 953: 'came',\n",
       " 1255: 'climax',\n",
       " 5383: 'refused',\n",
       " 3778: 'let',\n",
       " 6653: 'texas',\n",
       " 3391: 'instruments',\n",
       " 4005: 'manual',\n",
       " 939: 'calculator',\n",
       " 5102: 'probably',\n",
       " 485: 'bamboozled',\n",
       " 2737: 'generation',\n",
       " 5453: 'required',\n",
       " 921: 'buy',\n",
       " 1509: 'cost',\n",
       " 3559: 'judy',\n",
       " 3515: 'jetsons',\n",
       " 3701: 'laptop',\n",
       " 2693: 'future',\n",
       " 2869: 'graph',\n",
       " 6639: 'tesla',\n",
       " 4404: 'need',\n",
       " 1239: 'clean',\n",
       " 5109: 'procrastinator',\n",
       " 257: 'anymore',\n",
       " 36: 'according',\n",
       " 1746: 'deepakoprah',\n",
       " 7213: 'way',\n",
       " 2885: 'grew',\n",
       " 4757: 'past',\n",
       " 4026: 'married',\n",
       " 3985: 'man',\n",
       " 3922: 'lucky',\n",
       " 2926: 'guy',\n",
       " 2724: 'gave',\n",
       " 2599: 'forever',\n",
       " 1693: 'dated',\n",
       " 3894: 'losers',\n",
       " 3899: 'lots',\n",
       " 6024: 'skaters',\n",
       " 7180: 'wanna',\n",
       " 2901: 'grownass',\n",
       " 7343: 'woman',\n",
       " 6361: 'stop',\n",
       " 1696: 'dating',\n",
       " 7010: 'unless',\n",
       " 7165: 'wake',\n",
       " 4066: 'mattress',\n",
       " 3644: 'kitchen',\n",
       " 6678: 'theyre',\n",
       " 5860: 'sexy',\n",
       " 4647: 'outside',\n",
       " 3982: 'malt',\n",
       " 3833: 'liquor',\n",
       " 3250: 'husband',\n",
       " 4144: 'met',\n",
       " 7229: 'wedding',\n",
       " 3091: 'hes',\n",
       " 3882: 'looking',\n",
       " 3742: 'league',\n",
       " 5696: 'saw',\n",
       " 4556: 'oh',\n",
       " 2808: 'god',\n",
       " 6682: 'thing',\n",
       " 3747: 'learned',\n",
       " 387: 'attending',\n",
       " 3013: 'harvard',\n",
       " 909: 'business',\n",
       " 5721: 'school',\n",
       " 6843: 'trap',\n",
       " 346: 'ass',\n",
       " 2814: 'going',\n",
       " 6844: 'trapped',\n",
       " 3360: 'initially',\n",
       " 3643: 'kissing',\n",
       " 2458: 'fifth',\n",
       " 1692: 'date',\n",
       " 7025: 'unusual',\n",
       " 1849: 'did',\n",
       " 5193: 'purpose',\n",
       " 3653: 'knew',\n",
       " 1028: 'catch',\n",
       " 2834: 'gotta',\n",
       " 3974: 'make',\n",
       " 2057: 'dude',\n",
       " 575: 'believe',\n",
       " 5777: 'secret',\n",
       " 2710: 'garden',\n",
       " 5168: 'public',\n",
       " 4726: 'park',\n",
       " 3203: 'hosted',\n",
       " 5388: 'reggae',\n",
       " 2445: 'fests',\n",
       " 33: 'accidentally',\n",
       " 3154: 'homeless',\n",
       " 3122: 'hipsters',\n",
       " 6366: 'store',\n",
       " 7041: 'urban',\n",
       " 4642: 'outfitters',\n",
       " 6683: 'things',\n",
       " 1425: 'confusing',\n",
       " 3121: 'hipster',\n",
       " 541: 'beard',\n",
       " 2398: 'fashion',\n",
       " 7189: 'warmth',\n",
       " 2990: 'happened',\n",
       " 3850: 'living',\n",
       " 831: 'broad',\n",
       " 1708: 'daylight',\n",
       " 1143: 'chemistry',\n",
       " 3093: 'hey',\n",
       " 7201: 'wassup',\n",
       " 7145: 'volvo',\n",
       " 2035: 'drop',\n",
       " 2036: 'dropped',\n",
       " 2816: 'golden',\n",
       " 2720: 'gate',\n",
       " 7205: 'watched',\n",
       " 5621: 'run',\n",
       " 4165: 'middle',\n",
       " 2651: 'friends',\n",
       " 335: 'asian',\n",
       " 5917: 'shocked',\n",
       " 7056: 'usually',\n",
       " 336: 'asianamerican',\n",
       " 7346: 'women',\n",
       " 7220: 'wear',\n",
       " 3632: 'kinda',\n",
       " 2794: 'glasses',\n",
       " 4608: 'opinions',\n",
       " 7282: 'white',\n",
       " 2058: 'dudes',\n",
       " 4413: 'neighborhood',\n",
       " 3972: 'major',\n",
       " 1216: 'city',\n",
       " 196: 'america',\n",
       " 7435: 'yoko',\n",
       " 4592: 'ono',\n",
       " 2357: 'factory',\n",
       " 7263: 'whats',\n",
       " 7404: 'wrong',\n",
       " 2424: 'feel',\n",
       " 4886: 'picturesque',\n",
       " 7251: 'wes',\n",
       " 214: 'anderson',\n",
       " 4309: 'movie',\n",
       " 6593: 'teach',\n",
       " 1482: 'cool',\n",
       " 6427: 'stuff',\n",
       " 7148: 'voting',\n",
       " 5360: 'recycling',\n",
       " 1933: 'disturbing',\n",
       " 1947: 'documentaries',\n",
       " 3414: 'introduce',\n",
       " 3204: 'hot',\n",
       " 3175: 'hookin',\n",
       " 4085: 'mean',\n",
       " 3976: 'makes',\n",
       " 5041: 'powerful',\n",
       " 2104: 'eats',\n",
       " 5206: 'pussy',\n",
       " 19: 'absorbing',\n",
       " 5099: 'privilege',\n",
       " 2225: 'entitlement',\n",
       " 4257: 'money',\n",
       " 3142: 'hole',\n",
       " 7151: 'vulnerable',\n",
       " 1609: 'crush',\n",
       " 3032: 'head',\n",
       " 4251: 'moment',\n",
       " 3623: 'kill',\n",
       " 788: 'brains',\n",
       " 1330: 'colonize',\n",
       " 1331: 'colonizer',\n",
       " 3660: 'knowbut',\n",
       " 4024: 'marriage',\n",
       " 4444: 'nice',\n",
       " 6148: 'somebody',\n",
       " 5252: 'race',\n",
       " 85: 'advantage',\n",
       " 5258: 'racist',\n",
       " 5699: 'say',\n",
       " 2331: 'explain',\n",
       " 2948: 'halffilipino',\n",
       " 2950: 'halfjapanese',\n",
       " 2946: 'halfchinese',\n",
       " 2953: 'halfvietnamese',\n",
       " 6207: 'spend',\n",
       " 4822: 'percent',\n",
       " 5912: 'shitting',\n",
       " 3667: 'korean',\n",
       " 192: 'amazing',\n",
       " 3908: 'love',\n",
       " 869: 'built',\n",
       " 3663: 'knowmy',\n",
       " 777: 'boyfriend',\n",
       " 1615: 'cuban',\n",
       " 4155: 'mexican',\n",
       " 2927: 'guys',\n",
       " 299: 'arent',\n",
       " 6989: 'underrated',\n",
       " 5854: 'sexiest',\n",
       " 2938: 'hair',\n",
       " 4402: 'neck',\n",
       " 3978: 'making',\n",
       " 1961: 'dolphin',\n",
       " 6104: 'smooth',\n",
       " 6068: 'slip',\n",
       " 6059: 'slide',\n",
       " 664: 'black',\n",
       " 2503: 'fish',\n",
       " 6731: 'tilikum',\n",
       " 553: 'bed',\n",
       " 4597: 'oohwee',\n",
       " 4138: 'mess',\n",
       " 3517: 'jewish',\n",
       " 5361: 'red',\n",
       " 3349: 'inflamed',\n",
       " 339: 'ask',\n",
       " 2303: 'exfoliated',\n",
       " 6758: 'today',\n",
       " 3490: 'jdate',\n",
       " 3879: 'loofah',\n",
       " 6659: 'thanks',\n",
       " 5611: 'rug',\n",
       " 898: 'burn',\n",
       " 420: 'avi',\n",
       " 4542: 'odor',\n",
       " 6090: 'smell',\n",
       " 5475: 'responsibility',\n",
       " 6972: 'umami',\n",
       " 2532: 'flavor',\n",
       " 1347: 'comes',\n",
       " 2660: 'fromi',\n",
       " 7023: 'unspoken',\n",
       " 6991: 'understanding',\n",
       " 2947: 'halffancy',\n",
       " 2951: 'halfjungle',\n",
       " 1856: 'difference',\n",
       " 2385: 'fancy',\n",
       " 337: 'asians',\n",
       " 1172: 'chinese',\n",
       " 3483: 'japanese',\n",
       " 3202: 'host',\n",
       " 4573: 'olympics',\n",
       " 3571: 'jungle',\n",
       " 1906: 'diseases',\n",
       " 1858: 'different',\n",
       " 2097: 'east',\n",
       " 1298: 'coast',\n",
       " 5098: 'private',\n",
       " 4942: 'playing',\n",
       " 3678: 'lacrosse',\n",
       " 3748: 'learning',\n",
       " 3713: 'latin',\n",
       " 1148: 'chess',\n",
       " 5612: 'rugby',\n",
       " 2469: 'filipino',\n",
       " 1001: 'carlton',\n",
       " 1850: 'didnt',\n",
       " 7117: 'vietnamese',\n",
       " 1695: 'dates',\n",
       " 6780: 'took',\n",
       " 5478: 'restaurant',\n",
       " 7252: 'west',\n",
       " 3891: 'los',\n",
       " 221: 'angeles',\n",
       " 944: 'called',\n",
       " 4860: 'pho',\n",
       " 408: 'authentic',\n",
       " 5312: 'read',\n",
       " 7426: 'yelp',\n",
       " 4504: 'number',\n",
       " 5775: 'second',\n",
       " 524: 'bathroom',\n",
       " 3766: 'legit',\n",
       " 1982: 'double',\n",
       " 6487: 'supply',\n",
       " 1275: 'closet',\n",
       " 2697: 'gallons',\n",
       " 675: 'bleach',\n",
       " 373: 'atm',\n",
       " 3944: 'machine',\n",
       " 2859: 'grandma',\n",
       " 2795: 'glaucoma',\n",
       " 4375: 'napping',\n",
       " 1494: 'corner',\n",
       " 7160: 'wait',\n",
       " 6268: 'staff',\n",
       " 3750: 'leave',\n",
       " 1716: 'deaf',\n",
       " 2183: 'emotionally',\n",
       " 22: 'abused',\n",
       " 6790: 'total',\n",
       " 616: 'big',\n",
       " 3119: 'hippies',\n",
       " 453: 'backpack',\n",
       " 6179: 'southeast',\n",
       " 334: 'asia',\n",
       " 7432: 'yoga',\n",
       " 436: 'ayahuasca',\n",
       " 1066: 'ceremonies',\n",
       " 5983: 'silent',\n",
       " 5487: 'retreats',\n",
       " 4779: 'pay',\n",
       " 5957: 'shut',\n",
       " 7235: 'weekend',\n",
       " 2804: 'glutenfree',\n",
       " 4089: 'means',\n",
       " 799: 'bread',\n",
       " 6582: 'tastes',\n",
       " 2638: 'freerange',\n",
       " 1152: 'chewbacca',\n",
       " 3774: 'lesbian',\n",
       " 6694: 'thousand',\n",
       " 1662: 'daily',\n",
       " 2451: 'fiber',\n",
       " 6232: 'spoken',\n",
       " 7359: 'word',\n",
       " 4962: 'poetry',\n",
       " 5224: 'queef',\n",
       " 5913: 'shitty',\n",
       " 4960: 'poem',\n",
       " 6489: 'supporting',\n",
       " 932: 'caitlyn',\n",
       " 3500: 'jenner',\n",
       " 2690: 'funny',\n",
       " 3120: 'hippydippy',\n",
       " 1956: 'doing',\n",
       " 3310: 'impression',\n",
       " 5754: 'scrolls',\n",
       " 7171: 'wall',\n",
       " 859: 'buddha',\n",
       " 4894: 'piggy',\n",
       " 494: 'bank',\n",
       " 4890: 'pier',\n",
       " 3306: 'imports',\n",
       " 5156: 'providing',\n",
       " 2820: 'good',\n",
       " 2440: 'feng',\n",
       " 5955: 'shui',\n",
       " 3209: 'house',\n",
       " 7420: 'years',\n",
       " 6116: 'sneaking',\n",
       " 6510: 'suspicion',\n",
       " 5142: 'propose',\n",
       " 5071: 'pressuring',\n",
       " 7153: 'wacky',\n",
       " 3420: 'intuition',\n",
       " 5141: 'proposals',\n",
       " 7363: 'work',\n",
       " 3319: 'incept',\n",
       " 3266: 'idea',\n",
       " 4002: 'mans',\n",
       " 4755: 'passively',\n",
       " 1951: 'doesnt',\n",
       " 4139: 'message',\n",
       " 2343: 'extremely',\n",
       " 114: 'aggressively',\n",
       " 6697: 'threaten',\n",
       " 60: 'actually',\n",
       " 3752: 'leaving',\n",
       " 4563: 'old',\n",
       " 3709: 'late',\n",
       " 4435: 'new',\n",
       " 6292: 'start',\n",
       " 3998: 'manipulation',\n",
       " 1653: 'cycle',\n",
       " 6341: 'stick',\n",
       " 2570: 'focus',\n",
       " 6845: 'trapping',\n",
       " 4360: 'nag',\n",
       " 4651: 'outta',\n",
       " 7215: 'weak',\n",
       " 1047: 'caves',\n",
       " 2762: 'gets',\n",
       " 2420: 'fed',\n",
       " 2483: 'fine',\n",
       " 4028: 'marry',\n",
       " 5143: 'proposed',\n",
       " 3880: 'look',\n",
       " 2284: 'exact',\n",
       " 5535: 'ring',\n",
       " 7182: 'wanted',\n",
       " 4070: 'maybe',\n",
       " 4906: 'pinterest',\n",
       " 4685: 'page',\n",
       " 5827: 'sent',\n",
       " 594: 'best',\n",
       " 2648: 'friend',\n",
       " 5817: 'send',\n",
       " 4907: 'pinterested',\n",
       " 2206: 'engaged',\n",
       " 5683: 'saturday',\n",
       " 761: 'bought',\n",
       " 2014: 'dress',\n",
       " 2579: 'following',\n",
       " 6913: 'tuesday',\n",
       " 6874: 'tried',\n",
       " 5316: 'ready',\n",
       " 5540: 'ripe',\n",
       " 5592: 'rotten',\n",
       " 488: 'banana',\n",
       " 6502: 'surprised',\n",
       " 4555: 'offstage',\n",
       " 1386: 'completely',\n",
       " 5350: 'recognize',\n",
       " 4836: 'personality',\n",
       " 6137: 'soft',\n",
       " 4511: 'nurturing',\n",
       " 1963: 'domestic',\n",
       " 7254: 'weve',\n",
       " 3464: 'ive',\n",
       " 4678: 'packed',\n",
       " 3927: 'lunch',\n",
       " 6003: 'single',\n",
       " 3063: 'hed',\n",
       " 1790: 'dependent',\n",
       " 2847: 'graduated',\n",
       " 2422: 'feed',\n",
       " 2823: 'goodness',\n",
       " 3052: 'heart',\n",
       " 3426: 'investment',\n",
       " 2479: 'financial',\n",
       " 5314: 'reading',\n",
       " 737: 'book',\n",
       " 5899: 'sheryl',\n",
       " 5663: 'sandberg',\n",
       " 5900: 'shes',\n",
       " 1478: 'coo',\n",
       " 2352: 'facebook',\n",
       " 7407: 'wrote',\n",
       " 5533: 'riled',\n",
       " 993: 'careers',\n",
       " 6566: 'talking',\n",
       " 1078: 'challenge',\n",
       " 6011: 'sit',\n",
       " 6551: 'table',\n",
       " 5543: 'rise',\n",
       " 3743: 'lean',\n",
       " 3804: 'lie',\n",
       " 7181: 'want',\n",
       " 2436: 'feminism',\n",
       " 3525: 'job',\n",
       " 7047: 'used',\n",
       " 6085: 'smart',\n",
       " 1459: 'continue',\n",
       " 2063: 'dumb',\n",
       " 1060: 'century',\n",
       " 2912: 'guess',\n",
       " 6313: 'stay',\n",
       " 6106: 'snacks',\n",
       " 7204: 'watch',\n",
       " 2160: 'ellen',\n",
       " 6432: 'stupid',\n",
       " 5317: 'real',\n",
       " 652: 'bitch',\n",
       " 5614: 'ruined',\n",
       " 2316: 'expected',\n",
       " 3048: 'hear',\n",
       " 4869: 'phrase',\n",
       " 1983: 'doubleincome',\n",
       " 3211: 'household',\n",
       " 7035: 'upset',\n",
       " 1362: 'comments',\n",
       " 4615: 'options',\n",
       " 2635: 'free',\n",
       " 7022: 'unscheduled',\n",
       " 7024: 'unsupervised',\n",
       " 3305: 'importantly',\n",
       " 6233: 'sponsored',\n",
       " 5911: 'shittier',\n",
       " 2582: 'food',\n",
       " 2086: 'earn',\n",
       " 3453: 'ita',\n",
       " 7167: 'walk',\n",
       " 6677: 'theyll',\n",
       " 3558: 'judgmental',\n",
       " 3214: 'housewives',\n",
       " 6383: 'street',\n",
       " 3213: 'housewife',\n",
       " 7169: 'walking',\n",
       " 4042: 'massages',\n",
       " 3926: 'lululemon',\n",
       " 4710: 'pants',\n",
       " 2745: 'genius',\n",
       " 5485: 'retiredi',\n",
       " 7400: 'write',\n",
       " 2643: 'fresh',\n",
       " 712: 'boat',\n",
       " 6: 'abc',\n",
       " 2876: 'great',\n",
       " 1545: 'coworkers',\n",
       " 7402: 'writing',\n",
       " 6632: 'terms',\n",
       " 3526: 'jobs',\n",
       " 4551: 'office',\n",
       " 6031: 'skin',\n",
       " 5770: 'seat',\n",
       " 7046: 'use',\n",
       " 6763: 'toilet',\n",
       " 4712: 'paper',\n",
       " 1538: 'cover',\n",
       " 6736: 'times',\n",
       " 5641: 'sadass',\n",
       " 4083: 'meal',\n",
       " 4583: 'oneply',\n",
       " 5195: 'purposely',\n",
       " 1860: 'difficult',\n",
       " 5177: 'pull',\n",
       " 6902: 'try',\n",
       " 5301: 'ration',\n",
       " 1368: 'communist',\n",
       " 2127: 'effective',\n",
       " 1762: 'dehydrates',\n",
       " 7325: 'wiping',\n",
       " 1802: 'desert',\n",
       " 3840: 'literally',\n",
       " 6190: 'spat',\n",
       " 1709: 'days',\n",
       " 115: 'ago',\n",
       " 3941: 'macgyver',\n",
       " 442: 'baby',\n",
       " 7323: 'wipe',\n",
       " 4243: 'moisten',\n",
       " 448: 'backfired',\n",
       " 2486: 'fingers',\n",
       " 833: 'broke',\n",
       " 1865: 'digitally',\n",
       " 6344: 'stimulated',\n",
       " 1969: 'doo',\n",
       " 2487: 'finish',\n",
       " 5629: 'rushed',\n",
       " 4718: 'paranoid',\n",
       " 5920: 'shoes',\n",
       " 6988: 'underneath',\n",
       " 6274: 'stall',\n",
       " 1534: 'courtneys',\n",
       " 3837: 'listening',\n",
       " 7162: 'waiting',\n",
       " 6737: 'timing',\n",
       " 3246: 'hurry',\n",
       " 2427: 'feels',\n",
       " 934: 'caked',\n",
       " 3875: 'long',\n",
       " 1683: 'dare',\n",
       " 5741: 'scratch',\n",
       " 6994: 'underwear',\n",
       " 2195: 'end',\n",
       " 3883: 'looks',\n",
       " 2826: 'goonies',\n",
       " 4320: 'muffle',\n",
       " 7381: 'worry',\n",
       " 7093: 'velocity',\n",
       " 6256: 'squeeze',\n",
       " 1131: 'cheeks',\n",
       " 6494: 'sure',\n",
       " 6071: 'slow',\n",
       " 6320: 'steady',\n",
       " 4675: 'pace',\n",
       " 7016: 'unpredictable',\n",
       " 4471: 'noise',\n",
       " 6453: 'suddenly',\n",
       " 2245: 'escapes',\n",
       " 827: 'brings',\n",
       " 1745: 'deep',\n",
       " 5873: 'shame',\n",
       " 694: 'blow',\n",
       " 2106: 'echo',\n",
       " 5498: 'reverberate',\n",
       " 2200: 'ends',\n",
       " 2958: 'hallways',\n",
       " 7207: 'watching',\n",
       " 4430: 'netflix',\n",
       " 3436: 'ipad',\n",
       " 746: 'boring',\n",
       " 5448: 'repressed',\n",
       " 5910: 'shits',\n",
       " 3836: 'listen',\n",
       " 4959: 'podcasts',\n",
       " 4924: 'planet',\n",
       " 7184: 'wantyou',\n",
       " 1929: 'distracting',\n",
       " 3892: 'lose',\n",
       " 5469: 'respect',\n",
       " 3141: 'holds',\n",
       " 6168: 'sort',\n",
       " 1576: 'credence',\n",
       " 3049: 'heard',\n",
       " 4424: 'nerve',\n",
       " 489: 'bananas',\n",
       " 2882: 'green',\n",
       " 478: 'ballet',\n",
       " 2531: 'flats',\n",
       " 2406: 'fatherinlaw',\n",
       " 6013: 'sitdown',\n",
       " 5344: 'recently',\n",
       " 6564: 'talk',\n",
       " 5155: 'provide',\n",
       " 1162: 'children',\n",
       " 5100: 'privileged',\n",
       " 1160: 'childhood',\n",
       " 6615: 'telling',\n",
       " 1470: 'conversation',\n",
       " 6155: 'son',\n",
       " 6990: 'understand',\n",
       " 2087: 'earning',\n",
       " 1185: 'choose',\n",
       " 5477: 'rest',\n",
       " 1193: 'chose',\n",
       " 5128: 'promise',\n",
       " 2085: 'early',\n",
       " 5486: 'retirement',\n",
       " 4090: 'meant',\n",
       " 1855: 'dieting',\n",
       " 2103: 'eating',\n",
       " 2647: 'fried',\n",
       " 1155: 'chicken',\n",
       " 2676: 'fulfilling',\n",
       " 1813: 'destiny',\n",
       " 1210: 'circle',\n",
       " 2348: 'eyelashes',\n",
       " 4315: 'mrs',\n",
       " 4681: 'pacman',\n",
       " 3779: 'lets',\n",
       " 5362: 'redecoratei',\n",
       " 1912: 'disgusting',\n",
       " 4841: 'pervert',\n",
       " 2893: 'gross',\n",
       " 2476: 'filthy',\n",
       " 227: 'animal',\n",
       " 6293: 'started',\n",
       " 5007: 'porn',\n",
       " 7443: 'young',\n",
       " 106: 'age',\n",
       " 2993: 'happens',\n",
       " 7452: 'yyou',\n",
       " 5967: 'sicker',\n",
       " 3287: 'images',\n",
       " 1564: 'crave',\n",
       " 3406: 'internet',\n",
       " 7441: 'youi',\n",
       " 3274: 'idiot',\n",
       " 5322: 'realize',\n",
       " 7248: 'went',\n",
       " 1557: 'craigslist',\n",
       " 5023: 'posted',\n",
       " 6742: 'tiny',\n",
       " 2434: 'female',\n",
       " 5788: 'seeking',\n",
       " 205: 'anal',\n",
       " 1559: 'crash',\n",
       " 3979: 'male',\n",
       " 3039: 'heads',\n",
       " 7008: 'universe',\n",
       " 5998: 'simultaneously',\n",
       " 2335: 'explode',\n",
       " 2629: 'freaked',\n",
       " 5710: 'scared',\n",
       " 4689: 'pain',\n",
       " 130: 'aint',\n",
       " 7212: 'wax',\n",
       " 2347: 'eyebrows',\n",
       " 6170: 'sorts',\n",
       " 1569: 'crazy',\n",
       " 1841: 'dick',\n",
       " 2276: 'evil',\n",
       " 5780: 'secrets',\n",
       " 3806: 'lies',\n",
       " 5832: 'sephora',\n",
       " 5208: 'puts',\n",
       " 6685: 'thinking',\n",
       " 1657: 'dad',\n",
       " 1699: 'dave',\n",
       " 2268: 'eventually',\n",
       " 1090: 'change',\n",
       " 1122: 'cheat',\n",
       " 3403: 'interesting',\n",
       " 3143: 'holes',\n",
       " 7442: 'youll',\n",
       " 5859: 'sexually',\n",
       " 54: 'active',\n",
       " 5479: 'result',\n",
       " 3841: 'little',\n",
       " 651: 'bit',\n",
       " 6390: 'stretched',\n",
       " 2478: 'finally',\n",
       " 2433: 'felt',\n",
       " 1087: 'chance',\n",
       " 3959: 'magical',\n",
       " 2391: 'fantasy',\n",
       " 5020: 'possible',\n",
       " 1896: 'discover',\n",
       " 5145: 'prostate',\n",
       " 1433: 'conqueror',\n",
       " 3026: 'havent',\n",
       " 6856: 'treat',\n",
       " 6778: 'tonight',\n",
       " 3843: 'live',\n",
       " 7437: 'yolo',\n",
       " 6113: 'sneak',\n",
       " 5204: 'pushpush',\n",
       " 6926: 'tushtush',\n",
       " 363: 'atari',\n",
       " 5464: 'resistance',\n",
       " 6259: 'squirmy',\n",
       " 7378: 'wormy',\n",
       " 6717: 'thumb',\n",
       " 6445: 'succeed',\n",
       " 2726: 'gay',\n",
       " 2413: 'fear',\n",
       " 6924: 'turns',\n",
       " 2249: 'especially',\n",
       " 4148: 'metamorphosizes',\n",
       " 4944: 'pleasure',\n",
       " 2345: 'eye',\n",
       " 1897: 'discovered',\n",
       " 4464: 'nirvana',\n",
       " 3681: 'lady',\n",
       " 1266: 'clit',\n",
       " 2350: 'eyes',\n",
       " 3889: 'lord',\n",
       " 5534: 'rim',\n",
       " 6998: 'unfortunately',\n",
       " 2633: 'freaky',\n",
       " 340: 'asked',\n",
       " 6188: 'spank',\n",
       " 1950: 'does',\n",
       " 21: 'abuse',\n",
       " 6411: 'strongheaded',\n",
       " 3904: 'loudmouthed',\n",
       " 1062: 'ceos',\n",
       " 5595: 'roughed',\n",
       " 7183: 'wants',\n",
       " 1465: 'control',\n",
       " 5545: 'risk',\n",
       " 1180: 'choke',\n",
       " 6745: 'tired',\n",
       " 749: 'boss',\n",
       " 555: 'bedroom',\n",
       " 4288: 'motherfucker',\n",
       " 3297: 'impact',\n",
       " 486: 'ban',\n",
       " 751: 'bossy',\n",
       " 2153: 'elementary',\n",
       " 5723: 'schools',\n",
       " 5856: 'sexist',\n",
       " 779: 'boys',\n",
       " 3385: 'instead',\n",
       " 5702: 'saying',\n",
       " 6491: 'supposed',\n",
       " 2300: 'executive',\n",
       " 3739: 'leadership',\n",
       " 6030: 'skills',\n",
       " 5597: 'roundabout',\n",
       " 1626: 'cunt',\n",
       " 2480: 'financially',\n",
       " 1068: 'certain',\n",
       " 4964: 'point',\n",
       " 1526: 'couple',\n",
       " 1349: 'comfortably',\n",
       " 94: 'afford',\n",
       " 6056: 'sliced',\n",
       " 3991: 'mango',\n",
       " 2583: 'foods',\n",
       " 3326: 'income',\n",
       " 784: 'bracket',\n",
       " 6403: 'striving',\n",
       " 7449: 'youve',\n",
       " 4368: 'named',\n",
       " 4466: 'noah',\n",
       " 5337: 'rebecca',\n",
       " 3647: 'kiwi',\n",
       " 1680: 'danielle',\n",
       " 4903: 'pineapple',\n",
       " 10: 'able',\n",
       " 6407: 'stroll',\n",
       " 5971: 'sidewalk',\n",
       " 5222: 'quarter',\n",
       " 5087: 'princess',\n",
       " 7048: 'useful',\n",
       " 88: 'advice',\n",
       " 840: 'brothers',\n",
       " 6010: 'sisters',\n",
       " 4692: 'paintballing',\n",
       " 7116: 'vietnam',\n",
       " 7101: 'veteran',\n",
       " 5845: 'seven',\n",
       " 2944: 'half',\n",
       " 4269: 'months',\n",
       " 5056: 'pregnant',\n",
       " 5297: 'rare',\n",
       " 1351: 'comic',\n",
       " 4825: 'perform',\n",
       " 1352: 'comics',\n",
       " 2736: 'generally',\n",
       " 1016: 'case',\n",
       " 7234: 'week',\n",
       " 443: 'babys',\n",
       " 4887: 'piece',\n",
       " 239: 'annoying',\n",
       " 1661: 'dads',\n",
       " 397: 'audience',\n",
       " 3106: 'hilarious',\n",
       " 3272: 'identify',\n",
       " 2376: 'fame',\n",
       " 6529: 'swells',\n",
       " 5401: 'relatable',\n",
       " 6452: 'sudden',\n",
       " 1102: 'chapping',\n",
       " 4462: 'nipples',\n",
       " 2423: 'feeding',\n",
       " 7221: 'wearing',\n",
       " 2661: 'frozen',\n",
       " 1837: 'diaper',\n",
       " 4407: 'needs',\n",
       " 5950: 'shredding',\n",
       " 2989: 'happen',\n",
       " 6280: 'standup',\n",
       " 6447: 'successful',\n",
       " 2383: 'famous',\n",
       " 1894: 'discouraged',\n",
       " 3027: 'having',\n",
       " 3619: 'kid',\n",
       " 3686: 'lame',\n",
       " 6314: 'stayathome',\n",
       " 2010: 'dream',\n",
       " 6970: 'ultimate',\n",
       " 7348: 'won',\n",
       " 3659: 'knowanother',\n",
       " 1895: 'discouraging',\n",
       " 6849: 'travel',\n",
       " 379: 'attached',\n",
       " 1854: 'dies',\n",
       " 1756: 'definitely',\n",
       " 2908: 'guaranteed',\n",
       " 633: 'billion',\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "# id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"fucking\" + 0.006*\"fuck\" + 0.006*\"shit\" + 0.005*\"say\" + 0.005*\"theyre\" + 0.005*\"didnt\" + 0.005*\"thing\" + 0.005*\"hes\" + 0.005*\"going\" + 0.005*\"theres\"'),\n",
       " (1,\n",
       "  '0.006*\"dad\" + 0.005*\"oh\" + 0.004*\"going\" + 0.004*\"say\" + 0.004*\"hey\" + 0.004*\"shes\" + 0.004*\"mom\" + 0.004*\"shit\" + 0.004*\"want\" + 0.004*\"day\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"fucking\" + 0.007*\"fuck\" + 0.006*\"cause\" + 0.006*\"theyre\" + 0.006*\"thing\" + 0.005*\"theres\" + 0.005*\"say\" + 0.005*\"really\" + 0.005*\"day\" + 0.005*\"good\"'),\n",
       " (1,\n",
       "  '0.006*\"dad\" + 0.006*\"love\" + 0.005*\"want\" + 0.005*\"say\" + 0.005*\"going\" + 0.005*\"oh\" + 0.004*\"hey\" + 0.004*\"shes\" + 0.004*\"did\" + 0.004*\"little\"'),\n",
       " (2,\n",
       "  '0.009*\"fucking\" + 0.009*\"shit\" + 0.006*\"didnt\" + 0.005*\"hes\" + 0.005*\"fuck\" + 0.005*\"say\" + 0.004*\"did\" + 0.004*\"want\" + 0.004*\"theyre\" + 0.004*\"come\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"cause\" + 0.006*\"says\" + 0.005*\"really\" + 0.005*\"say\" + 0.005*\"mean\" + 0.004*\"goes\" + 0.004*\"oh\" + 0.004*\"way\" + 0.004*\"didnt\" + 0.004*\"id\"'),\n",
       " (1,\n",
       "  '0.010*\"shit\" + 0.005*\"fuck\" + 0.005*\"lot\" + 0.005*\"didnt\" + 0.005*\"oh\" + 0.005*\"man\" + 0.005*\"ahah\" + 0.005*\"fucking\" + 0.004*\"black\" + 0.004*\"hes\"'),\n",
       " (2,\n",
       "  '0.008*\"love\" + 0.007*\"bo\" + 0.007*\"stuff\" + 0.007*\"repeat\" + 0.005*\"want\" + 0.005*\"cos\" + 0.005*\"fucking\" + 0.005*\"eye\" + 0.005*\"um\" + 0.005*\"contact\"'),\n",
       " (3,\n",
       "  '0.010*\"fucking\" + 0.007*\"fuck\" + 0.007*\"shit\" + 0.006*\"say\" + 0.006*\"theyre\" + 0.006*\"going\" + 0.006*\"thing\" + 0.005*\"hes\" + 0.005*\"went\" + 0.005*\"theres\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Topic Modeling - Attempt # 2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies and gentlemen please welcome to the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank you thank you thank you san francisco th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>all right thank you thank you very much thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>bo what old macdonald had a farm e i e i o and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>this is dave he tells dirty jokes for a living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>whats up davis whats up im home i had to bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies and gentlemen please welcome to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies and gentlemen welcome joe rogan  wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>all right petunia wish me luck out there you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>introfade the music out lets roll hold there l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thank you thanks thank you guys hey se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello hello how you doing great thank you wow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies and gentlemen please welcome to the sta...\n",
       "anthony  thank you thank you thank you san francisco th...\n",
       "bill      all right thank you thank you very much thank...\n",
       "bo       bo what old macdonald had a farm e i e i o and...\n",
       "dave     this is dave he tells dirty jokes for a living...\n",
       "hasan      whats up davis whats up im home i had to bri...\n",
       "jim         ladies and gentlemen please welcome to the ...\n",
       "joe         ladies and gentlemen welcome joe rogan  wha...\n",
       "john     all right petunia wish me luck out there you w...\n",
       "louis    introfade the music out lets roll hold there l...\n",
       "mike     wow hey thank you thanks thank you guys hey se...\n",
       "ricky    hello hello how you doing great thank you wow ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\yrobi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen stage ali hi thank hello na s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank thank people i em i francisco city world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>thank thank pleasure georgia area oasis i june...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>macdonald farm e i o farm pig e i i snort macd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>jokes living stare work profound train thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>whats davis whats home i netflix la york i son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen stage mr jim jefferies thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe fuck thanks phone fuckfac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>petunia thats hello hello chicago thank crowd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>music lets lights lights thank i i place place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks look insane years everyone i id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello thank fuck thank im gon youre weve money...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen stage ali hi thank hello na s...\n",
       "anthony  thank thank people i em i francisco city world...\n",
       "bill     thank thank pleasure georgia area oasis i june...\n",
       "bo       macdonald farm e i o farm pig e i i snort macd...\n",
       "dave     jokes living stare work profound train thought...\n",
       "hasan    whats davis whats home i netflix la york i son...\n",
       "jim      ladies gentlemen stage mr jim jefferies thank ...\n",
       "joe      ladies gentlemen joe fuck thanks phone fuckfac...\n",
       "john     petunia thats hello hello chicago thank crowd ...\n",
       "louis    music lets lights lights thank i i place place...\n",
       "mike     wow hey thanks look insane years everyone i id...\n",
       "ricky    hello thank fuck thank im gon youre weve money..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>abuse</th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zee</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 4633 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aah  abc  abcs  ability  \\\n",
       "ali                 0                 0          0    0    1     0        0   \n",
       "anthony             0                 0          0    0    0     0        0   \n",
       "bill                0                 0          0    0    0     1        0   \n",
       "bo                  1                 1          1    0    0     0        1   \n",
       "dave                0                 0          0    0    0     0        0   \n",
       "hasan               0                 0          0    0    0     0        0   \n",
       "jim                 0                 0          0    0    0     0        0   \n",
       "joe                 0                 0          0    0    0     0        0   \n",
       "john                0                 0          0    0    0     0        0   \n",
       "louis               0                 0          0    3    0     0        0   \n",
       "mike                0                 0          0    0    0     0        0   \n",
       "ricky               0                 0          0    0    0     0        1   \n",
       "\n",
       "         abortion  abortions  abuse  ...  yummy  ze  zealand  zee  zeppelin  \\\n",
       "ali             0          0      0  ...      0   0        0    0         0   \n",
       "anthony         2          0      0  ...      0   0       10    0         0   \n",
       "bill            0          0      0  ...      0   1        0    0         0   \n",
       "bo              0          0      0  ...      0   0        0    0         0   \n",
       "dave            0          1      0  ...      0   0        0    0         0   \n",
       "hasan           0          0      0  ...      0   0        0    1         0   \n",
       "jim             0          0      0  ...      0   0        0    0         0   \n",
       "joe             0          0      1  ...      0   0        0    0         0   \n",
       "john            0          0      0  ...      0   0        0    0         0   \n",
       "louis           0          0      0  ...      0   0        0    0         0   \n",
       "mike            0          0      0  ...      0   0        0    0         2   \n",
       "ricky           0          0      0  ...      1   0        0    0         0   \n",
       "\n",
       "         zillion  zombie  zombies  zoo  éclair  \n",
       "ali            0       1        0    0       0  \n",
       "anthony        0       0        0    0       0  \n",
       "bill           1       1        1    0       0  \n",
       "bo             0       0        0    0       0  \n",
       "dave           0       0        0    0       0  \n",
       "hasan          0       0        0    0       0  \n",
       "jim            0       0        0    0       0  \n",
       "joe            0       0        0    0       0  \n",
       "john           0       0        0    0       1  \n",
       "louis          0       0        0    0       0  \n",
       "mike           0       0        0    0       0  \n",
       "ricky          0       0        0    1       0  \n",
       "\n",
       "[12 rows x 4633 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"thing\" + 0.010*\"life\" + 0.010*\"shit\" + 0.009*\"fuck\" + 0.008*\"day\" + 0.008*\"man\" + 0.008*\"guy\" + 0.008*\"cause\" + 0.008*\"hes\" + 0.007*\"gon\"'),\n",
       " (1,\n",
       "  '0.008*\"day\" + 0.007*\"way\" + 0.006*\"man\" + 0.006*\"thing\" + 0.006*\"hes\" + 0.006*\"shit\" + 0.006*\"dad\" + 0.006*\"lot\" + 0.005*\"life\" + 0.005*\"years\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"day\" + 0.009*\"thing\" + 0.008*\"man\" + 0.008*\"cause\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.007*\"things\" + 0.006*\"shit\" + 0.006*\"guy\" + 0.006*\"hes\"'),\n",
       " (1,\n",
       "  '0.014*\"life\" + 0.011*\"thing\" + 0.008*\"hes\" + 0.007*\"cause\" + 0.007*\"course\" + 0.006*\"kids\" + 0.006*\"tit\" + 0.006*\"guy\" + 0.005*\"shit\" + 0.005*\"way\"'),\n",
       " (2,\n",
       "  '0.010*\"shit\" + 0.008*\"hes\" + 0.007*\"life\" + 0.007*\"man\" + 0.007*\"dad\" + 0.007*\"day\" + 0.006*\"dude\" + 0.005*\"fuck\" + 0.005*\"thing\" + 0.005*\"stuff\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"man\" + 0.009*\"fuck\" + 0.008*\"day\" + 0.008*\"shit\" + 0.008*\"thing\" + 0.007*\"women\" + 0.006*\"guy\" + 0.006*\"hes\" + 0.006*\"lot\" + 0.006*\"way\"'),\n",
       " (1,\n",
       "  '0.001*\"life\" + 0.001*\"shit\" + 0.001*\"man\" + 0.001*\"thing\" + 0.001*\"hes\" + 0.001*\"dad\" + 0.000*\"lot\" + 0.000*\"fuck\" + 0.000*\"cause\" + 0.000*\"way\"'),\n",
       " (2,\n",
       "  '0.012*\"thing\" + 0.011*\"day\" + 0.009*\"life\" + 0.009*\"hes\" + 0.008*\"way\" + 0.007*\"shit\" + 0.007*\"joke\" + 0.007*\"kids\" + 0.007*\"guy\" + 0.007*\"years\"'),\n",
       " (3,\n",
       "  '0.008*\"shit\" + 0.008*\"cause\" + 0.007*\"life\" + 0.007*\"dad\" + 0.007*\"fuck\" + 0.007*\"house\" + 0.007*\"man\" + 0.006*\"lot\" + 0.006*\"shes\" + 0.006*\"day\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen welcome stage ali wong hi wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank san francisco thank good people surprise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>right thank thank pleasure greater atlanta geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>old macdonald farm e i i o farm pig e i i snor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>dirty jokes living stare most hard work profou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>whats davis whats im home i netflix special la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen welcome stage mr jim jefferie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe fuck san francisco thanks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>right petunia august thats good right hello he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>music lets lights lights thank much i i i nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks hey seattle nice look crazy ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello great thank fuck thank lovely welcome im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen welcome stage ali wong hi wel...\n",
       "anthony  thank san francisco thank good people surprise...\n",
       "bill     right thank thank pleasure greater atlanta geo...\n",
       "bo       old macdonald farm e i i o farm pig e i i snor...\n",
       "dave     dirty jokes living stare most hard work profou...\n",
       "hasan    whats davis whats im home i netflix special la...\n",
       "jim      ladies gentlemen welcome stage mr jim jefferie...\n",
       "joe      ladies gentlemen joe fuck san francisco thanks...\n",
       "john     right petunia august thats good right hello he...\n",
       "louis    music lets lights lights thank much i i i nice...\n",
       "mike     wow hey thanks hey seattle nice look crazy ins...\n",
       "ricky    hello great thank fuck thank lovely welcome im..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zee</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 5584 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aah  abc  abcs  \\\n",
       "ali           0             0                 0          0    0    1     0   \n",
       "anthony       0             0                 0          0    0    0     0   \n",
       "bill          1             0                 0          0    0    0     1   \n",
       "bo            0             1                 1          1    0    0     0   \n",
       "dave          0             0                 0          0    0    0     0   \n",
       "hasan         0             0                 0          0    0    0     0   \n",
       "jim           0             0                 0          0    0    0     0   \n",
       "joe           0             0                 0          0    0    0     0   \n",
       "john          0             0                 0          0    0    0     0   \n",
       "louis         0             0                 0          0    3    0     0   \n",
       "mike          0             0                 0          0    0    0     0   \n",
       "ricky         0             0                 0          0    0    0     0   \n",
       "\n",
       "         ability  abject  able  ...  ze  zealand  zee  zeppelin  zero  \\\n",
       "ali            0       0     2  ...   0        0    0         0     0   \n",
       "anthony        0       0     0  ...   0       10    0         0     0   \n",
       "bill           0       0     1  ...   1        0    0         0     0   \n",
       "bo             1       0     0  ...   0        0    0         0     1   \n",
       "dave           0       0     0  ...   0        0    0         0     0   \n",
       "hasan          0       0     1  ...   0        0    2         0     0   \n",
       "jim            0       0     1  ...   0        0    0         0     0   \n",
       "joe            0       0     2  ...   0        0    0         0     0   \n",
       "john           0       0     3  ...   0        0    0         0     0   \n",
       "louis          0       0     1  ...   0        0    0         0     0   \n",
       "mike           0       0     0  ...   0        0    0         2     0   \n",
       "ricky          1       1     2  ...   0        0    0         0     0   \n",
       "\n",
       "         zillion  zombie  zombies  zoo  éclair  \n",
       "ali            0       1        0    0       0  \n",
       "anthony        0       0        0    0       0  \n",
       "bill           1       1        1    0       0  \n",
       "bo             0       0        0    0       0  \n",
       "dave           0       0        0    0       0  \n",
       "hasan          0       0        0    0       0  \n",
       "jim            0       0        0    0       0  \n",
       "joe            0       0        0    0       0  \n",
       "john           0       0        0    0       1  \n",
       "louis          0       0        0    0       0  \n",
       "mike           0       0        0    0       0  \n",
       "ricky          0       0        0    1       0  \n",
       "\n",
       "[12 rows x 5584 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"door\" + 0.003*\"friend\" + 0.003*\"wife\" + 0.003*\"dick\" + 0.002*\"york\" + 0.002*\"hasan\" + 0.002*\"jenny\" + 0.002*\"clinton\"'),\n",
       " (1,\n",
       "  '0.007*\"joke\" + 0.004*\"jokes\" + 0.003*\"bo\" + 0.003*\"ass\" + 0.003*\"um\" + 0.003*\"guns\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"anthony\" + 0.003*\"party\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"mom\" + 0.005*\"parents\" + 0.004*\"joke\" + 0.004*\"hasan\" + 0.004*\"clinton\" + 0.003*\"york\" + 0.003*\"cow\" + 0.003*\"nuts\" + 0.003*\"tit\" + 0.002*\"food\"'),\n",
       " (1,\n",
       "  '0.005*\"joke\" + 0.004*\"bo\" + 0.003*\"guns\" + 0.003*\"jokes\" + 0.003*\"um\" + 0.003*\"repeat\" + 0.003*\"son\" + 0.003*\"ahah\" + 0.003*\"mad\" + 0.003*\"ass\"'),\n",
       " (2,\n",
       "  '0.005*\"jenny\" + 0.003*\"husband\" + 0.003*\"ok\" + 0.003*\"gun\" + 0.003*\"morning\" + 0.003*\"sudden\" + 0.003*\"accident\" + 0.003*\"sense\" + 0.002*\"love\" + 0.002*\"marriage\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"bo\" + 0.003*\"jenny\" + 0.003*\"repeat\" + 0.003*\"ahah\" + 0.003*\"gay\" + 0.003*\"comedy\" + 0.003*\"friend\" + 0.003*\"door\" + 0.003*\"dick\" + 0.003*\"love\"'),\n",
       " (1,\n",
       "  '0.013*\"joke\" + 0.010*\"anthony\" + 0.007*\"grandma\" + 0.006*\"mad\" + 0.006*\"jokes\" + 0.006*\"shark\" + 0.005*\"mom\" + 0.005*\"san\" + 0.004*\"zealand\" + 0.004*\"today\"'),\n",
       " (2,\n",
       "  '0.004*\"joke\" + 0.004*\"hasan\" + 0.004*\"parents\" + 0.003*\"guns\" + 0.003*\"ass\" + 0.003*\"class\" + 0.003*\"mom\" + 0.003*\"dog\" + 0.003*\"nuts\" + 0.003*\"tit\"'),\n",
       " (3,\n",
       "  '0.007*\"mom\" + 0.007*\"clinton\" + 0.005*\"husband\" + 0.005*\"cow\" + 0.004*\"ok\" + 0.004*\"wife\" + 0.003*\"pregnant\" + 0.003*\"parents\" + 0.003*\"movie\" + 0.003*\"ass\"')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"mom\" + 0.004*\"gun\" + 0.004*\"ass\" + 0.004*\"hasan\" + 0.004*\"guns\" + 0.003*\"class\" + 0.003*\"son\" + 0.003*\"parents\" + 0.003*\"husband\" + 0.003*\"brown\"'),\n",
       " (1,\n",
       "  '0.009*\"ahah\" + 0.007*\"nigga\" + 0.006*\"gay\" + 0.004*\"son\" + 0.004*\"oj\" + 0.004*\"ghetto\" + 0.004*\"motherfucker\" + 0.004*\"young\" + 0.004*\"kevin\" + 0.003*\"mad\"'),\n",
       " (2,\n",
       "  '0.008*\"joke\" + 0.005*\"jokes\" + 0.004*\"bo\" + 0.003*\"eye\" + 0.003*\"repeat\" + 0.003*\"dead\" + 0.003*\"anthony\" + 0.003*\"nuts\" + 0.003*\"contact\" + 0.003*\"brain\"'),\n",
       " (3,\n",
       "  '0.006*\"clinton\" + 0.006*\"jenny\" + 0.005*\"parents\" + 0.005*\"friend\" + 0.005*\"mom\" + 0.004*\"cow\" + 0.004*\"wife\" + 0.004*\"john\" + 0.003*\"accident\" + 0.003*\"idea\"')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'ali'),\n",
       " (2, 'anthony'),\n",
       " (0, 'bill'),\n",
       " (2, 'bo'),\n",
       " (1, 'dave'),\n",
       " (0, 'hasan'),\n",
       " (0, 'jim'),\n",
       " (2, 'joe'),\n",
       " (3, 'john'),\n",
       " (2, 'louis'),\n",
       " (3, 'mike'),\n",
       " (2, 'ricky')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first pass of LDA, these kind of make sense.\n",
    "* Topic 0: mom, parents [Anthony, Hasan, Louis, Ricky]\n",
    "* Topic 1: husband, wife [Ali, John, Mike]\n",
    "* Topic 2: guns [Bill, Bo, Jim]\n",
    "* Topic 3: profanity [Dave, Joe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
